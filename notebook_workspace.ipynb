{
 "cells": [
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "#!pip install sqlalchemy psycopg2-binary minio weaviate-client pandas",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Cell 0: Service Endpoints & Credentials\n\n# Postgres (for Resolve or your own metadata DB)\nPG_HOST = \"localhost\"\nPG_PORT = 5432\nPG_DB   = \"resolveLibrary\"\nPG_USER = \"davinci\"\nPG_PASS = \"supersecret\"\n\n# MinIO (for object storage)\nMINIO_ENDPOINT = \"localhost:9000\"\nMINIO_ACCESS   = \"minio\"\nMINIO_SECRET   = \"minio123\"\n\n# Weaviate (vector store)\nWEAVIATE_URL   = \"http://localhost:8080\"\n\n# Example imports\nimport os, pandas as pd\nfrom sqlalchemy import create_engine\nfrom minio import Minio\nimport weaviate\n\n# Create clients\nengine = create_engine(f\"postgresql://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\")\nminio_client = Minio(MINIO_ENDPOINT, access_key=MINIO_ACCESS, secret_key=MINIO_SECRET, secure=False)\nweaviate_client = weaviate.Client(WEAVIATE_URL)\n\nprint(\"‚úÖ Clients initialized!\",\n      f\"Postgres: {PG_HOST}:{PG_PORT}/{PG_DB}\",\n      f\"MinIO: {MINIO_ENDPOINT}\",\n      f\"Weaviate: {WEAVIATE_URL}\", sep=\"\\n\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# If using `.env` and `requirements.txt`"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "!pip install --upgrade pip\n!pip install -r requirements.txt",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Cell 1: Load environment and initialize clients\nimport os\nfrom dotenv import load_dotenv\n\n# Load .env file\nload_dotenv(dotenv_path=\".env\")\n\n# Read into variables\nPG_HOST     = os.getenv(\"PG_HOST\")\nPG_PORT     = os.getenv(\"PG_PORT\")\nPG_DB       = os.getenv(\"PG_DB\")\nPG_USER     = os.getenv(\"PG_USER\")\nPG_PASS     = os.getenv(\"PG_PASS\")\n\nMINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\")\nMINIO_ACCESS   = os.getenv(\"MINIO_ACCESS\")\nMINIO_SECRET   = os.getenv(\"MINIO_SECRET\")\nMINIO_SECURE   = os.getenv(\"MINIO_SECURE\") == \"true\"\n\nWEAVIATE_URL = os.getenv(\"WEAVIATE_URL\")\n\n# Initialize clients\nfrom sqlalchemy import create_engine\nfrom minio import Minio\nimport weaviate\n\nengine = create_engine(f\"postgresql://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\")\nminio_client = Minio(\n    endpoint=MINIO_ENDPOINT,\n    access_key=MINIO_ACCESS,\n    secret_key=MINIO_SECRET,\n    secure=MINIO_SECURE\n)\nweaviate_client = weaviate.Client(url=WEAVIATE_URL)\n\nprint(\"‚úÖ Clients ready:\")\nprint(f\" ‚Ä¢ Postgres: {PG_USER}@{PG_HOST}:{PG_PORT}/{PG_DB}\")\nprint(f\" ‚Ä¢ MinIO: {MINIO_ENDPOINT} (secure={MINIO_SECURE})\")\nprint(f\" ‚Ä¢ Weaviate: {WEAVIATE_URL}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Video Batch Processing Notebook Structure\n\nThis is how your notebook cells should be organized when the `.ipynb` file is at the repo root:\n\n## File Structure Expected:\n\n```\nüìÅ your-repo/                    ‚Üê Repo root (where .ipynb is located)\n‚îú‚îÄ‚îÄ üìì batching_video_data.ipynb ‚Üê Your main notebook\n‚îú‚îÄ‚îÄ üìÅ scripts/                  ‚Üê Your existing scripts folder\n‚îú‚îÄ‚îÄ üìÅ Batches/                  ‚Üê Optional: traditional structure\n‚îî‚îÄ‚îÄ üìÅ ../                       ‚Üê Parent directory where video batches live\n    ‚îú‚îÄ‚îÄ üìÅ Z7V_1641/             ‚Üê Video batch (parent dir = batch name)\n    ‚îÇ   ‚îú‚îÄ‚îÄ üé¨ video1.mp4\n    ‚îÇ   ‚îî‚îÄ‚îÄ üé¨ video2.mp4\n    ‚îú‚îÄ‚îÄ üìÅ Z7V_1642/             ‚Üê Another batch\n    ‚îÇ   ‚îî‚îÄ‚îÄ üé¨ video3.mp4\n    ‚îî‚îÄ‚îÄ üìÅ StockFootage/         ‚Üê Your organized stock footage\n        ‚îî‚îÄ‚îÄ üìÅ Z7V_1643/\n            ‚îî‚îÄ‚îÄ üé¨ video5.mp4\n```\n\n## Cell Execution Order:\n\n1. **Cell 1**: Setup and imports - establishes repo root context\n1. **Cell 2**: Discovery functions - defines batch detection logic\n1. **Cell 3**: Run discovery - finds batches in current or parent directory\n1. **Cell 4**: Generate metadata - creates comprehensive batch information\n1. **Cell 5**: Create DataFrame - converts to pandas for analysis\n1. **Cell 6**: Save results - exports JSON and CSV files to repo root\n1. **Cell 7**: Interactive analysis - provides functions for exploration\n1. **Cell 8**: Utility functions - additional tools for processing\n\n## Key Features:\n\n- **Auto-detects repo location**: Uses `Path.cwd()` to establish repo root\n- **Smart directory scanning**: Checks current directory first, then parent\n- **Batch identification**: Groups videos by their parent directory names\n- **Comprehensive metadata**: Generates JSON with all batch information\n- **DataFrame creation**: Makes data analysis easy with pandas\n- **File output**: Saves results to repo root for version control\n- **Interactive tools**: Provides functions for exploring batches\n\n## Usage Pattern:\n\n1. Place notebook at repo root\n1. Run all cells sequentially\n1. Use interactive functions to analyze specific batches\n1. Generated files (`batch_metadata.json`, `video_inventory.csv`) stay in repo\n1. Can be deployed anywhere with the workspace setup scripts\n\nThis approach keeps your notebook portable while maintaining the ability to discover and process video batches regardless of where they‚Äôre located relative to your repo."
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Cell 1: Setup and Imports\nimport os\nimport json\nimport glob\nfrom pathlib import Path\nfrom collections import defaultdict\nimport hashlib\nfrom datetime import datetime\nimport pandas as pd\n\n# Set working directory to repo root (where this notebook is located)\nrepo_root = Path.cwd()\nprint(f\"üìÇ Repository root: {repo_root}\")\n\n# Video file extensions to look for\nVIDEO_EXTENSIONS = {'.mp4', '.mov', '.avi', '.mkv', '.m4v', '.wmv', '.flv', '.webm'}\n\nprint(\"‚úÖ Setup complete\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Cell 1.5: Configuration - Insert this between Cell 1 and Cell 2\n# =======================================================================\n# CONFIGURATION CELL - Customize your batch discovery behavior\n# =======================================================================\n\n# Target directories to scan (if empty, scans current and parent directories)\nTARGET_DIRECTORIES = [\n    # Examples (uncomment and modify as needed):\n    # \".\",                          # Current directory (repo root)\n    # \"..\",                         # Parent directory\n    # \"../StockFootage\",            # Specific path\n    # \"/path/to/video/storage\",     # Absolute path\n    # \"~/Videos/Batches\",           # Home directory relative\n]\n\n# Directory names to specifically include (if empty, includes all directories with videos)\nINCLUDE_BATCH_NAMES = [\n    # Examples (uncomment and modify as needed):\n    # \"Z7V_*\",                      # Wildcard pattern\n    # \"Batch*\",                     # Another wildcard\n    # \"StockFootage\",               # Specific directory name\n    # \"2024*\",                      # Year-based batches\n]\n\n# Directory names to ignore/exclude\nIGNORE_DIRECTORY_NAMES = [\n    # Common directories to skip\n    \".git\", \".vscode\", \"__pycache__\", \".DS_Store\", \"node_modules\",\n    \".ipynb_checkpoints\", \"venv\", \"env\", \".venv\",\n    \n    # Add your custom exclusions:\n    # \"temp\", \"backup\", \"old\", \"archive\", \"trash\",\n    # \"rendered\", \"exports\", \"thumbnails\",\n]\n\n# File size filters (optional)\nMIN_FILE_SIZE_MB = 0        # Minimum file size in MB (0 = no minimum)\nMAX_FILE_SIZE_GB = 50       # Maximum file size in GB (0 = no maximum, 50 = reasonable default)\n\n# Additional video extensions (beyond the defaults)\nADDITIONAL_VIDEO_EXTENSIONS = [\n    # Add any custom video formats you use:\n    # \".mxf\", \".prores\", \".dnxhd\", \".r3d\", \".braw\"\n]\n\n# Batch naming options\nBATCH_NAME_TRANSFORM = \"none\"  # Options: \"none\", \"uppercase\", \"lowercase\", \"title\"\nREMOVE_BATCH_PREFIXES = []     # Remove these prefixes from batch names, e.g., [\"Batch_\", \"Video_\"]\nREMOVE_BATCH_SUFFIXES = []     # Remove these suffixes from batch names, e.g., [\"_raw\", \"_temp\"]\n\n# Output file locations (relative to repo root or absolute paths)\nOUTPUT_DIRECTORY = \".\"         # Where to save metadata files (\".\" = repo root)\nMETADATA_FILENAME = \"batch_metadata.json\"\nINVENTORY_FILENAME = \"video_inventory.csv\"\nWORKSPACE_INFO_FILENAME = \"workspace_info.json\"\n\n# Session-specific metadata file (useful for temporary/local runs)\nSESSION_METADATA_LOCATION = None  # None = use OUTPUT_DIRECTORY, or specify custom path\n# Examples:\n# SESSION_METADATA_LOCATION = \"../session_data\"     # Parent directory\n# SESSION_METADATA_LOCATION = \"/tmp/video_session\"  # Temporary location\n# SESSION_METADATA_LOCATION = \"~/Desktop\"           # Desktop for easy access\n\nprint(\"‚öôÔ∏è Configuration loaded:\")\nprint(f\"   Target directories: {TARGET_DIRECTORIES or 'Auto-detect (current and parent)'}\")\nprint(f\"   Include patterns: {INCLUDE_BATCH_NAMES or 'All directories with videos'}\")\nprint(f\"   Ignore directories: {len(IGNORE_DIRECTORY_NAMES)} patterns\")\nprint(f\"   File size range: {MIN_FILE_SIZE_MB}MB - {MAX_FILE_SIZE_GB}GB\")\nprint(f\"   Additional video formats: {ADDITIONAL_VIDEO_EXTENSIONS or 'None'}\")\nprint(f\"   Output directory: {OUTPUT_DIRECTORY}\")\nprint(f\"   Session metadata: {SESSION_METADATA_LOCATION or 'Same as output directory'}\")\nprint(\"‚úÖ Ready to discover batches with custom settings\")\n\n# Helper functions for configuration\nimport fnmatch\nfrom pathlib import Path\n\ndef should_include_directory(dir_name, dir_path):\n    \"\"\"Check if directory should be included based on configuration\"\"\"\n    \n    # Check ignore list first\n    for ignore_pattern in IGNORE_DIRECTORY_NAMES:\n        if fnmatch.fnmatch(dir_name.lower(), ignore_pattern.lower()):\n            return False\n    \n    # If include list is specified, directory must match one of the patterns\n    if INCLUDE_BATCH_NAMES:\n        for include_pattern in INCLUDE_BATCH_NAMES:\n            if fnmatch.fnmatch(dir_name, include_pattern):\n                return True\n        return False  # Didn't match any include pattern\n    \n    return True  # No include list specified, so include by default\n\ndef transform_batch_name(batch_name):\n    \"\"\"Apply batch name transformations\"\"\"\n    name = batch_name\n    \n    # Remove prefixes\n    for prefix in REMOVE_BATCH_PREFIXES:\n        if name.startswith(prefix):\n            name = name[len(prefix):]\n    \n    # Remove suffixes\n    for suffix in REMOVE_BATCH_SUFFIXES:\n        if name.endswith(suffix):\n            name = name[:-len(suffix)]\n    \n    # Apply case transformation\n    if BATCH_NAME_TRANSFORM == \"uppercase\":\n        name = name.upper()\n    elif BATCH_NAME_TRANSFORM == \"lowercase\":\n        name = name.lower()\n    elif BATCH_NAME_TRANSFORM == \"title\":\n        name = name.title()\n    \n    return name\n\ndef should_include_video_file(file_path):\n    \"\"\"Check if video file meets size requirements\"\"\"\n    try:\n        size_bytes = file_path.stat().st_size\n        size_mb = size_bytes / (1024**2)\n        size_gb = size_bytes / (1024**3)\n        \n        if MIN_FILE_SIZE_MB > 0 and size_mb < MIN_FILE_SIZE_MB:\n            return False\n        \n        if MAX_FILE_SIZE_GB > 0 and size_gb > MAX_FILE_SIZE_GB:\n            return False\n        \n        return True\n    except:\n        return True  # If we can't check size, include it\n\ndef get_target_directories():\n    \"\"\"Get list of directories to scan based on configuration\"\"\"\n    if TARGET_DIRECTORIES:\n        # Use specified target directories\n        targets = []\n        for target in TARGET_DIRECTORIES:\n            target_path = Path(target).expanduser().resolve()\n            if target_path.exists() and target_path.is_dir():\n                targets.append(target_path)\n            else:\n                print(f\"‚ö†Ô∏è Target directory not found: {target}\")\n        return targets\n    else:\n        # Use default auto-detection logic\n        return [Path(\".\"), Path(\"..\")]\n\ndef get_output_path(filename):\n    \"\"\"Get full output path for a file based on configuration\"\"\"\n    if SESSION_METADATA_LOCATION:\n        # Use session-specific location\n        session_path = Path(SESSION_METADATA_LOCATION).expanduser().resolve()\n        session_path.mkdir(parents=True, exist_ok=True)\n        return session_path / filename\n    else:\n        # Use configured output directory\n        output_path = Path(OUTPUT_DIRECTORY).expanduser().resolve()\n        output_path.mkdir(parents=True, exist_ok=True)\n        return output_path / filename\n\ndef get_metadata_paths():\n    \"\"\"Get all configured output file paths\"\"\"\n    return {\n        'metadata': get_output_path(METADATA_FILENAME),\n        'inventory': get_output_path(INVENTORY_FILENAME),\n        'workspace_info': get_output_path(WORKSPACE_INFO_FILENAME)\n    }\n\n# Update video extensions with additional formats\nALL_VIDEO_EXTENSIONS = VIDEO_EXTENSIONS.union(set(ext.lower() for ext in ADDITIONAL_VIDEO_EXTENSIONS))\n\nprint(f\"üéØ Will scan {len(get_target_directories())} target directories\")\nprint(f\"üìπ Monitoring {len(ALL_VIDEO_EXTENSIONS)} video formats: {sorted(ALL_VIDEO_EXTENSIONS)}\")\nprint(f\"üíæ Output files will be saved to:\")\npaths = get_metadata_paths()\nfor file_type, path in paths.items():\n    print(f\"   {file_type}: {path}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# ============================================================================\n\n# Cell 2: Discovery Functions\ndef discover_video_batches(search_path=\".\"):\n    \"\"\"Discover video batches based on parent directory structure\"\"\"\n    search_root = Path(search_path)\n    batches = defaultdict(list)\n    \n    print(f\"üîç Scanning for video batches in: {search_root.absolute()}\")\n    \n    # Scan all subdirectories for video files\n    for item in search_root.iterdir():\n        if item.is_dir() and not item.name.startswith('.'):\n            batch_name = item.name\n            videos = []\n            \n            # Find video files in this directory\n            for video_file in item.iterdir():\n                if video_file.is_file() and video_file.suffix.lower() in VIDEO_EXTENSIONS:\n                    videos.append({\n                        'filename': video_file.name,\n                        'path': str(video_file.relative_to(search_root)),\n                        'full_path': str(video_file.absolute()),\n                        'size_bytes': video_file.stat().st_size,\n                        'size_mb': round(video_file.stat().st_size / (1024**2), 2),\n                        'size_gb': round(video_file.stat().st_size / (1024**3), 3),\n                        'modified': datetime.fromtimestamp(video_file.stat().st_mtime).isoformat(),\n                        'batch': batch_name\n                    })\n            \n            if videos:\n                batches[batch_name] = videos\n                print(f\"üìÅ Found batch '{batch_name}': {len(videos)} videos\")\n    \n    return dict(batches)\n\ndef scan_current_directory():\n    \"\"\"Scan current directory for video batches\"\"\"\n    return discover_video_batches(\".\")\n\ndef scan_parent_directory():\n    \"\"\"Scan parent directory for video batches (useful when notebook is in subdirectory)\"\"\"\n    return discover_video_batches(\"..\")\n\nprint(\"‚úÖ Discovery functions ready\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# ============================================================================\n\n# Cell 3: Run Discovery\n# First try current directory, then parent if no batches found\nbatches = scan_current_directory()\n\nif not batches:\n    print(\"üîÑ No batches in current directory, checking parent...\")\n    batches = scan_parent_directory()\n\nif not batches:\n    print(\"‚ùå No video batches found!\")\n    print(\"Expected structure:\")\n    print(\"  üìÅ BatchName1/\")\n    print(\"  ‚îú‚îÄ‚îÄ üé¨ video1.mp4\")\n    print(\"  ‚îî‚îÄ‚îÄ üé¨ video2.mp4\")\nelse:\n    print(f\"\\nüéØ Discovered {len(batches)} video batches\")\n    for batch_name in batches.keys():\n        print(f\"  üìÅ {batch_name}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# ============================================================================\n\n# Cell 4: Generate Comprehensive Metadata\ndef generate_batch_metadata(batches_dict):\n    \"\"\"Generate comprehensive metadata for all video batches\"\"\"\n    \n    metadata = {\n        'scan_time': datetime.now().isoformat(),\n        'notebook_location': str(repo_root.absolute()),\n        'total_batches': len(batches_dict),\n        'total_videos': sum(len(videos) for videos in batches_dict.values()),\n        'batches': {}\n    }\n    \n    for batch_name, videos in batches_dict.items():\n        total_size = sum(video['size_bytes'] for video in videos)\n        \n        metadata['batches'][batch_name] = {\n            'video_count': len(videos),\n            'total_size_bytes': total_size,\n            'total_size_mb': round(total_size / (1024**2), 2),\n            'total_size_gb': round(total_size / (1024**3), 2),\n            'videos': videos,\n            'batch_id': hashlib.md5(batch_name.encode()).hexdigest()[:8],\n            'first_video': videos[0]['filename'] if videos else None,\n            'last_modified': max(video['modified'] for video in videos) if videos else None\n        }\n    \n    return metadata\n\n# Generate metadata\nif batches:\n    metadata = generate_batch_metadata(batches)\n    print(f\"üìä Generated metadata for {metadata['total_batches']} batches\")\n    print(f\"   Total videos: {metadata['total_videos']}\")\n    print(f\"   Total size: {sum(b['total_size_gb'] for b in metadata['batches'].values()):.2f} GB\")\nelse:\n    metadata = None",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# ============================================================================\n\n# Cell 5: Create DataFrame for Analysis\ndef create_video_dataframe(batches_dict):\n    \"\"\"Create a pandas DataFrame from video batch data\"\"\"\n    all_videos = []\n    \n    for batch_name, videos in batches_dict.items():\n        for video in videos:\n            all_videos.append({\n                'batch_name': batch_name,\n                'filename': video['filename'],\n                'size_mb': video['size_mb'],\n                'size_gb': video['size_gb'],\n                'modified_date': pd.to_datetime(video['modified']),\n                'path': video['path'],\n                'full_path': video['full_path']\n            })\n    \n    return pd.DataFrame(all_videos)\n\nif batches:\n    df_videos = create_video_dataframe(batches)\n    print(f\"üìà Created DataFrame with {len(df_videos)} video records\")\n    print(\"\\nüîç Sample data:\")\n    display(df_videos.head())\n    \n    print(\"\\nüìä Batch summary:\")\n    batch_summary = df_videos.groupby('batch_name').agg({\n        'filename': 'count',\n        'size_gb': 'sum'\n    }).round(2)\n    batch_summary.columns = ['video_count', 'total_gb']\n    display(batch_summary)\nelse:\n    df_videos = None",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Cell 6: Save Results (Enhanced with configuration support)\ndef save_batch_data(metadata, df_videos=None):\n    \"\"\"Save batch metadata and optionally DataFrame to configured locations\"\"\"\n    \n    # Get configured file paths\n    paths = get_metadata_paths()\n    \n    # Save metadata JSON\n    with open(paths['metadata'], 'w') as f:\n        json.dump(metadata, f, indent=2)\n    print(f\"üíæ Saved metadata to: {paths['metadata']}\")\n    \n    # Save DataFrame as CSV if available\n    if df_videos is not None:\n        df_videos.to_csv(paths['inventory'], index=False)\n        print(f\"üíæ Saved video inventory to: {paths['inventory']}\")\n    \n    # Create workspace info\n    workspace_info = {\n        'workspace_type': 'video_batching',\n        'structure': 'parent_directory_batching',\n        'notebook_location': str(repo_root),\n        'session_metadata_location': str(SESSION_METADATA_LOCATION) if SESSION_METADATA_LOCATION else None,\n        'output_directory': str(OUTPUT_DIRECTORY),\n        'batches_discovered': list(metadata['batches'].keys()) if metadata else [],\n        'ready_for_processing': len(metadata['batches']) > 0 if metadata else False,\n        'last_scan': datetime.now().isoformat(),\n        'configuration': {\n            'target_directories': TARGET_DIRECTORIES,\n            'include_patterns': INCLUDE_BATCH_NAMES,\n            'ignore_patterns': IGNORE_DIRECTORY_NAMES,\n            'file_size_limits': {\n                'min_mb': MIN_FILE_SIZE_MB,\n                'max_gb': MAX_FILE_SIZE_GB\n            },\n            'batch_naming': {\n                'transform': BATCH_NAME_TRANSFORM,\n                'remove_prefixes': REMOVE_BATCH_PREFIXES,\n                'remove_suffixes': REMOVE_BATCH_SUFFIXES\n            }\n        }\n    }\n    \n    with open(paths['workspace_info'], 'w') as f:\n        json.dump(workspace_info, f, indent=2)\n    print(f\"üíæ Saved workspace info to: {paths['workspace_info']}\")\n    \n    return paths\n\ndef save_session_snapshot():\n    \"\"\"Save a quick session snapshot with timestamp\"\"\"\n    if metadata:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        snapshot_filename = f\"session_snapshot_{timestamp}.json\"\n        snapshot_path = get_output_path(snapshot_filename)\n        \n        snapshot_data = {\n            'timestamp': datetime.now().isoformat(),\n            'session_id': timestamp,\n            'batch_summary': {\n                batch_name: {\n                    'video_count': len(videos),\n                    'total_gb': round(sum(v['size_gb'] for v in videos), 2)\n                }\n                for batch_name, videos in batches.items()\n            },\n            'total_batches': len(batches),\n            'total_videos': sum(len(videos) for videos in batches.values()),\n            'configuration_used': {\n                'target_dirs': TARGET_DIRECTORIES,\n                'session_location': SESSION_METADATA_LOCATION\n            }\n        }\n        \n        with open(snapshot_path, 'w') as f:\n            json.dump(snapshot_data, f, indent=2)\n        print(f\"üì∏ Session snapshot saved to: {snapshot_path}\")\n        return snapshot_path\n    return None\n\nif metadata:\n    saved_files = save_batch_data(metadata, df_videos)\n    snapshot_file = save_session_snapshot()\n    print(\"‚úÖ All data saved successfully!\")\n    print(f\"üìÇ Files saved to: {paths['metadata'].parent}\")\nelse:\n    print(\"‚ùå No data to save - no batches found\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# ============================================================================\n\n# Cell 7: Interactive Analysis Functions\ndef analyze_batch(batch_name):\n    \"\"\"Analyze a specific batch\"\"\"\n    if batch_name not in batches:\n        print(f\"‚ùå Batch '{batch_name}' not found\")\n        return\n    \n    videos = batches[batch_name]\n    print(f\"üé¨ Analyzing batch: {batch_name}\")\n    print(f\"   Videos: {len(videos)}\")\n    print(f\"   Total size: {sum(v['size_gb'] for v in videos):.2f} GB\")\n    print(f\"   Average size: {sum(v['size_mb'] for v in videos) / len(videos):.1f} MB\")\n    \n    print(\"\\nüìã Video list:\")\n    for video in videos:\n        print(f\"   ‚Ä¢ {video['filename']} ({video['size_mb']} MB)\")\n\ndef list_all_batches():\n    \"\"\"List all discovered batches with summary info\"\"\"\n    if not batches:\n        print(\"‚ùå No batches found\")\n        return\n    \n    print(\"üìÅ All discovered batches:\")\n    for batch_name, videos in batches.items():\n        total_gb = sum(v['size_gb'] for v in videos)\n        print(f\"   {batch_name}: {len(videos)} videos, {total_gb:.2f} GB\")\n\n# Run interactive analysis\nif batches:\n    print(\"üéØ Interactive Analysis Ready!\")\n    print(\"\\nAvailable functions:\")\n    print(\"‚Ä¢ list_all_batches() - Show all batches\")\n    print(\"‚Ä¢ analyze_batch('batch_name') - Analyze specific batch\")\n    print(\"‚Ä¢ df_videos - Pandas DataFrame with all video data\")\n    print(\"‚Ä¢ metadata - Complete metadata dictionary\")\n    \n    # Auto-run summary\n    list_all_batches()\nelse:\n    print(\"‚ö†Ô∏è No batches available for analysis\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# ============================================================================\n\n# Cell 8: Utility Functions for Further Processing\ndef get_batch_paths(batch_name):\n    \"\"\"Get all video file paths for a specific batch\"\"\"\n    if batch_name not in batches:\n        return []\n    return [video['full_path'] for video in batches[batch_name]]\n\ndef get_largest_videos(n=5):\n    \"\"\"Get the N largest video files across all batches\"\"\"\n    if df_videos is not None:\n        return df_videos.nlargest(n, 'size_gb')[['batch_name', 'filename', 'size_gb']]\n    return None\n\ndef get_recent_videos(days=7):\n    \"\"\"Get videos modified in the last N days\"\"\"\n    if df_videos is not None:\n        cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=days)\n        recent = df_videos[df_videos['modified_date'] > cutoff_date]\n        return recent[['batch_name', 'filename', 'modified_date', 'size_gb']]\n    return None\n\nprint(\"üõ†Ô∏è Utility functions loaded:\")\nprint(\"‚Ä¢ get_batch_paths(batch_name) - Get file paths for batch\")\nprint(\"‚Ä¢ get_largest_videos(n=5) - Find largest videos\")\nprint(\"‚Ä¢ get_recent_videos(days=7) - Find recently modified videos\")\n\nif df_videos is not None:\n    print(f\"\\nüîç Example - 3 largest videos:\")\n    display(get_largest_videos(3))\n\nprint(\"\\n‚úÖ Video batch processing notebook ready!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Connecting to Davinci Resolve Network Project Postgres Data"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# /scripts/ingest_to_resolve.py\n\nimport json\nfrom pathlib import Path\n\n# 1. load your JSON metadata\nmeta = json.load(open(Path(__file__).parent.parent / \"batch_metadata.json\"))\n\n# 2. grab the Resolve scripting API\nimport DaVinciResolveScript as bmd\nresolve = bmd.scriptapp(\"Resolve\")\npm = resolve.GetProjectManager()\nproj = pm.LoadProject(\"StockFootage Library\")\nmp = proj.GetMediaPool()\n\n# 3. for each batch, make a bin & import its clips\nfor batch_name, info in meta[\"batches\"].items():\n    # create (or reuse) a bin\n    root = mp.GetRootFolder()\n    try:\n        bin = mp.AddSubFolder(root, batch_name)\n    except:\n        # already exists?\n        bin = next(b for b in root.GetSubFolders() if b.GetName() == batch_name)\n\n    # import media into that bin\n    paths = [v[\"full_path\"] for v in info[\"videos\"]]\n    mp.AddItemListToMediaPool(paths)\n\n    # tag each clip with metadata\n    for clip in bin.GetClipList():\n        # set the ‚ÄúScene‚Äù field to your batch name\n        clip.SetClipProperty(\"Scene\", batch_name)\n        # set the ‚ÄúComment‚Äù field to maybe your description or tags\n        clip.SetClipProperty(\"Comment\", f\"{info['video_count']} clips, total {info['total_size_gb']:.2f} GB\")\n\n# 4. save the project\nproj.Save()\nprint(\"‚úÖ Ingest + tagging complete.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Writing Blackbox Global's `metadata.xml`"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "import pandas as pd\n\n# Paths\ntemplate_path = \"BlackBox Metadata Template MAY 10 2024.xlsx\"\n\n# Load the main template sheet\ndf_meta = pd.read_excel(template_path, sheet_name=\"template\")\n\n# (Optional) load dropdown values for validation\ndf_dd = pd.read_excel(template_path, sheet_name=\"dataDropdown\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Example: assembling one row\nnew_row = {\n    \"File Name\": \"Z7V_1641/video1.mp4\",\n    \"Description (min 15, max 200 characters, must be least 5 words)\": \"A wide shot of...\",\n    \"Keywords (min 8, max 49, separated by comma, and no repetition)\": \"drone, landscape, 4k, nature, wide, aerial, motion, shot\",\n    \"Category (use dropdown menu)\": \"Nature\",\n    \"Editorial (use dropdown menu)\": False,\n    # etc‚Ä¶ fill every required column\n}\ndf_meta = df_meta.append(new_row, ignore_index=True)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "with pd.ExcelWriter(template_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n    df_meta.to_excel(writer, sheet_name=\"template\", index=False)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "import xml.etree.ElementTree as ET\n\nroot = ET.Element(\"MediaMetaData\")\nfor _, row in df_meta.iterrows():\n    clip = ET.SubElement(root, \"Clip\")\n    for col in df_meta.columns:\n        child = ET.SubElement(clip, col.replace(\" \", \"\"))\n        child.text = str(row[col])\ntree = ET.ElementTree(root)\ntree.write(\"metadata.xml\", encoding=\"utf-8\", xml_declaration=True)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## With this flow you‚Äôll have:\n\n- excel‚Äêdriven human‚Äêfriendly template for metadata entry & validation\n- automated sync from your Python pipeline into that template\n- XML export for BlackBox Global ingestion\n- Postgres backend feeding DaVinci Resolve‚Äôs shared library"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}