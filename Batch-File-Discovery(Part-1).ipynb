{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sqlalchemy psycopg2-binary minio weaviate-client pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skippable Service Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Service Endpoints & Credentials\n",
    "\n",
    "# Postgres (for Resolve or your own metadata DB)\n",
    "PG_HOST = \"192.168.0.21\"\n",
    "PG_PORT = 5432\n",
    "PG_DB   = \"resolveLibrary\"\n",
    "PG_USER = \"davinci\"\n",
    "PG_PASS = \"supersecret\"\n",
    "\n",
    "# MinIO (for object storage)\n",
    "MINIO_ENDPOINT = \"192.168.0.21:9000\"\n",
    "MINIO_ACCESS   = \"minio\"\n",
    "MINIO_SECRET   = \"minio123\"\n",
    "\n",
    "# Weaviate (vector store)\n",
    "WEAVIATE_URL   = \"http://192.168.0.21:8082\"\n",
    "\n",
    "# Example imports\n",
    "import os, pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from minio import Minio\n",
    "import weaviate\n",
    "\n",
    "# Create clients\n",
    "engine = create_engine(f\"postgresql://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\")\n",
    "minio_client = Minio(MINIO_ENDPOINT, access_key=MINIO_ACCESS, secret_key=MINIO_SECRET, secure=False)\n",
    "weaviate_client = weaviate.Client(WEAVIATE_URL)\n",
    "\n",
    "print(\"‚úÖ Clients initialized!\",\n",
    "      f\"Postgres: {PG_HOST}:{PG_PORT}/{PG_DB}\",\n",
    "      f\"MinIO: {MINIO_ENDPOINT}\",\n",
    "      f\"Weaviate: {WEAVIATE_URL}\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If using `.env` and `requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Load environment and initialize clients\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "# Read into variables\n",
    "PG_HOST     = os.getenv(\"PG_HOST\")\n",
    "PG_PORT     = os.getenv(\"PG_PORT\")\n",
    "PG_DB       = os.getenv(\"PG_DB\")\n",
    "PG_USER     = os.getenv(\"PG_USER\")\n",
    "PG_PASS     = os.getenv(\"PG_PASS\")\n",
    "\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\")\n",
    "MINIO_ACCESS   = os.getenv(\"MINIO_ACCESS\")\n",
    "MINIO_SECRET   = os.getenv(\"MINIO_SECRET\")\n",
    "MINIO_SECURE   = os.getenv(\"MINIO_SECURE\") == \"true\"\n",
    "\n",
    "WEAVIATE_URL = os.getenv(\"WEAVIATE_URL\")\n",
    "\n",
    "# Initialize clients\n",
    "from sqlalchemy import create_engine\n",
    "from minio import Minio\n",
    "import weaviate\n",
    "\n",
    "engine = create_engine(f\"postgresql://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\")\n",
    "minio_client = Minio(\n",
    "    endpoint=MINIO_ENDPOINT,\n",
    "    access_key=MINIO_ACCESS,\n",
    "    secret_key=MINIO_SECRET,\n",
    "    secure=MINIO_SECURE\n",
    ")\n",
    "weaviate_client = weaviate.Client(url=WEAVIATE_URL)\n",
    "\n",
    "print(\"‚úÖ Clients ready:\")\n",
    "print(f\" ‚Ä¢ Postgres: {PG_USER}@{PG_HOST}:{PG_PORT}/{PG_DB}\")\n",
    "print(f\" ‚Ä¢ MinIO: {MINIO_ENDPOINT} (secure={MINIO_SECURE})\")\n",
    "print(f\" ‚Ä¢ Weaviate: {WEAVIATE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning of Mapping Video Batches From Local Target Storage\n",
    "\n",
    "### Video Batch Processing Notebook Structure\n",
    "\n",
    "This is how your notebook cells should be organized when the `.ipynb` file is at the repo root:\n",
    "\n",
    "### File Structure Expected:\n",
    "\n",
    "```\n",
    "üìÅ your-repo/                    ‚Üê Repo root (where .ipynb is located)\n",
    "‚îú‚îÄ‚îÄ üìì batching_video_data.ipynb ‚Üê Your main notebook\n",
    "‚îú‚îÄ‚îÄ üìÅ scripts/                  ‚Üê Your existing scripts folder\n",
    "‚îú‚îÄ‚îÄ üìÅ Batches/                  ‚Üê Optional: traditional structure\n",
    "‚îî‚îÄ‚îÄ üìÅ ../                       ‚Üê Parent directory where video batches live\n",
    "    ‚îú‚îÄ‚îÄ üìÅ Z7V_1641/             ‚Üê Video batch (parent dir = batch name)\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ üé¨ video1.mp4\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ üé¨ video2.mp4\n",
    "    ‚îú‚îÄ‚îÄ üìÅ Z7V_1642/             ‚Üê Another batch\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ üé¨ video3.mp4\n",
    "    ‚îî‚îÄ‚îÄ üìÅ StockFootage/         ‚Üê Your organized stock footage\n",
    "        ‚îî‚îÄ‚îÄ üìÅ Z7V_1643/\n",
    "            ‚îî‚îÄ‚îÄ üé¨ video5.mp4\n",
    "```\n",
    "\n",
    "## Cell Execution Order:\n",
    "\n",
    "1. **Cell 1**: Setup and imports - establishes repo root context\n",
    "1. **Cell 2**: Discovery functions - defines batch detection logic\n",
    "1. **Cell 3**: Run discovery - finds batches in current or parent directory\n",
    "1. **Cell 4**: Generate metadata - creates comprehensive batch information\n",
    "1. **Cell 5**: Create DataFrame - converts to pandas for analysis\n",
    "1. **Cell 6**: Save results - exports JSON and CSV files to repo root\n",
    "1. **Cell 7**: Interactive analysis - provides functions for exploration\n",
    "1. **Cell 8**: Utility functions - additional tools for processing\n",
    "\n",
    "## Key Features:\n",
    "\n",
    "- **Auto-detects repo location**: Uses `Path.cwd()` to establish repo root\n",
    "- **Smart directory scanning**: Checks current directory first, then parent\n",
    "- **Batch identification**: Groups videos by their parent directory names\n",
    "- **Comprehensive metadata**: Generates JSON with all batch information\n",
    "- **DataFrame creation**: Makes data analysis easy with pandas\n",
    "- **File output**: Saves results to repo root for version control\n",
    "- **Interactive tools**: Provides functions for exploring batches\n",
    "\n",
    "## Usage Pattern:\n",
    "\n",
    "1. Place notebook at repo root\n",
    "1. Run all cells sequentially\n",
    "1. Use interactive functions to analyze specific batches\n",
    "1. Generated files (`batch_metadata.json`, `video_inventory.csv`) stay in repo\n",
    "1. Can be deployed anywhere with the workspace setup scripts\n",
    "\n",
    "This approach keeps your notebook portable while maintaining the ability to discover and process video batches regardless of where they‚Äôre located relative to your repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Mapped Drive Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['well_pump', 'woodbench_hardware_development']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Assuming your mapped drive `B:` starts at \\\\192.168.0.25\\B\n",
    "repo_root = Path(\"B:/Video/StockFootage/Batches\")\n",
    "\n",
    "print(repo_root.exists())\n",
    "print([d.name for d in repo_root.iterdir() if d.is_dir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning for video batches in: B:\\Video\\StockFootage\\Batches\n",
      "üìÅ Found batch 'well_pump': 16 videos\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def resolve_smb_batches():\n",
    "    smb_path = Path(\"B:/Video/StockFootage/Batches\")\n",
    "    assert smb_path.exists(), \"‚ùå SMB path not mounted or unavailable\"\n",
    "    return smb_path\n",
    "\n",
    "repo_root = resolve_smb_batches()\n",
    "batches = discover_video_batches(repo_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Repository root: C:\\Users\\david\\JupyterLab\\stock-video-metadata\n",
      "‚úÖ Setup complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Set working directory to repo root (where this notebook is located)\n",
    "repo_root = Path.cwd()\n",
    "print(f\"üìÇ Repository root: {repo_root}\")\n",
    "\n",
    "# Video file extensions to look for\n",
    "VIDEO_EXTENSIONS = {'.mp4', '.mov', '.avi', '.mkv', '.m4v', '.wmv', '.flv', '.webm'}\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 1.5: Configuration - Insert this between Cell 1 and Cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuration loaded:\n",
      "   Target directories: Auto-detect (current and parent)\n",
      "   Include patterns: All directories with videos\n",
      "   Ignore directories: 9 patterns\n",
      "   File size range: 0MB - 50GB\n",
      "   Additional video formats: None\n",
      "   Output directory: .\n",
      "   Session metadata: Same as output directory\n",
      "‚úÖ Ready to discover batches with custom settings\n",
      "üéØ Will scan 2 target directories\n",
      "üìπ Monitoring 8 video formats: ['.avi', '.flv', '.m4v', '.mkv', '.mov', '.mp4', '.webm', '.wmv']\n",
      "üíæ Output files will be saved to:\n",
      "   metadata: C:\\Users\\david\\JupyterLab\\stock-video-metadata\\batch_metadata.json\n",
      "   inventory: C:\\Users\\david\\JupyterLab\\stock-video-metadata\\video_inventory.csv\n",
      "   workspace_info: C:\\Users\\david\\JupyterLab\\stock-video-metadata\\workspace_info.json\n"
     ]
    }
   ],
   "source": [
    "=======================================================================\n",
    "# CONFIGURATION CELL - Customize your batch discovery behavior\n",
    "# =======================================================================\n",
    "\n",
    "# Target directories to scan (if empty, scans current and parent directories)\n",
    "TARGET_DIRECTORIES = [\n",
    "    # Examples (uncomment and modify as needed):\n",
    "    # \".\",                          # Current directory (repo root)\n",
    "    # \"..\",                         # Parent directory\n",
    "    # \"../StockFootage\",            # Specific path\n",
    "    # \"/path/to/video/storage\",     # Absolute path\n",
    "    # \"~/Videos/Batches\",           # Home directory relative\n",
    "]\n",
    "\n",
    "# Directory names to specifically include (if empty, includes all directories with videos)\n",
    "INCLUDE_BATCH_NAMES = [\n",
    "    # Examples (uncomment and modify as needed):\n",
    "    # \"Z7V_*\",                      # Wildcard pattern\n",
    "    # \"Batch*\",                     # Another wildcard\n",
    "    # \"StockFootage\",               # Specific directory name\n",
    "    # \"2024*\",                      # Year-based batches\n",
    "]\n",
    "\n",
    "# Directory names to ignore/exclude\n",
    "IGNORE_DIRECTORY_NAMES = [\n",
    "    # Common directories to skip\n",
    "    \".git\", \".vscode\", \"__pycache__\", \".DS_Store\", \"node_modules\",\n",
    "    \".ipynb_checkpoints\", \"venv\", \"env\", \".venv\",\n",
    "    \n",
    "    # Add your custom exclusions:\n",
    "    # \"temp\", \"backup\", \"old\", \"archive\", \"trash\",\n",
    "    # \"rendered\", \"exports\", \"thumbnails\",\n",
    "]\n",
    "\n",
    "# File size filters (optional)\n",
    "MIN_FILE_SIZE_MB = 0        # Minimum file size in MB (0 = no minimum)\n",
    "MAX_FILE_SIZE_GB = 50       # Maximum file size in GB (0 = no maximum, 50 = reasonable default)\n",
    "\n",
    "# Additional video extensions (beyond the defaults)\n",
    "ADDITIONAL_VIDEO_EXTENSIONS = [\n",
    "    # Add any custom video formats you use:\n",
    "    # \".mxf\", \".prores\", \".dnxhd\", \".r3d\", \".braw\"\n",
    "]\n",
    "\n",
    "# Batch naming options\n",
    "BATCH_NAME_TRANSFORM = \"none\"  # Options: \"none\", \"uppercase\", \"lowercase\", \"title\"\n",
    "REMOVE_BATCH_PREFIXES = []     # Remove these prefixes from batch names, e.g., [\"Batch_\", \"Video_\"]\n",
    "REMOVE_BATCH_SUFFIXES = []     # Remove these suffixes from batch names, e.g., [\"_raw\", \"_temp\"]\n",
    "\n",
    "# Output file locations (relative to repo root or absolute paths)\n",
    "OUTPUT_DIRECTORY = \".\"         # Where to save metadata files (\".\" = repo root)\n",
    "METADATA_FILENAME = \"batch_metadata.json\"\n",
    "INVENTORY_FILENAME = \"video_inventory.csv\"\n",
    "WORKSPACE_INFO_FILENAME = \"workspace_info.json\"\n",
    "\n",
    "# Session-specific metadata file (useful for temporary/local runs)\n",
    "SESSION_METADATA_LOCATION = None  # None = use OUTPUT_DIRECTORY, or specify custom path\n",
    "# Examples:\n",
    "# SESSION_METADATA_LOCATION = \"../session_data\"     # Parent directory\n",
    "# SESSION_METADATA_LOCATION = \"/tmp/video_session\"  # Temporary location\n",
    "# SESSION_METADATA_LOCATION = \"~/Desktop\"           # Desktop for easy access\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration loaded:\")\n",
    "print(f\"   Target directories: {TARGET_DIRECTORIES or 'Auto-detect (current and parent)'}\")\n",
    "print(f\"   Include patterns: {INCLUDE_BATCH_NAMES or 'All directories with videos'}\")\n",
    "print(f\"   Ignore directories: {len(IGNORE_DIRECTORY_NAMES)} patterns\")\n",
    "print(f\"   File size range: {MIN_FILE_SIZE_MB}MB - {MAX_FILE_SIZE_GB}GB\")\n",
    "print(f\"   Additional video formats: {ADDITIONAL_VIDEO_EXTENSIONS or 'None'}\")\n",
    "print(f\"   Output directory: {OUTPUT_DIRECTORY}\")\n",
    "print(f\"   Session metadata: {SESSION_METADATA_LOCATION or 'Same as output directory'}\")\n",
    "print(\"‚úÖ Ready to discover batches with custom settings\")\n",
    "\n",
    "# Helper functions for configuration\n",
    "import fnmatch\n",
    "from pathlib import Path\n",
    "\n",
    "def should_include_directory(dir_name, dir_path):\n",
    "    \"\"\"Check if directory should be included based on configuration\"\"\"\n",
    "    \n",
    "    # Check ignore list first\n",
    "    for ignore_pattern in IGNORE_DIRECTORY_NAMES:\n",
    "        if fnmatch.fnmatch(dir_name.lower(), ignore_pattern.lower()):\n",
    "            return False\n",
    "    \n",
    "    # If include list is specified, directory must match one of the patterns\n",
    "    if INCLUDE_BATCH_NAMES:\n",
    "        for include_pattern in INCLUDE_BATCH_NAMES:\n",
    "            if fnmatch.fnmatch(dir_name, include_pattern):\n",
    "                return True\n",
    "        return False  # Didn't match any include pattern\n",
    "    \n",
    "    return True  # No include list specified, so include by default\n",
    "\n",
    "def transform_batch_name(batch_name):\n",
    "    \"\"\"Apply batch name transformations\"\"\"\n",
    "    name = batch_name\n",
    "    \n",
    "    # Remove prefixes\n",
    "    for prefix in REMOVE_BATCH_PREFIXES:\n",
    "        if name.startswith(prefix):\n",
    "            name = name[len(prefix):]\n",
    "    \n",
    "    # Remove suffixes\n",
    "    for suffix in REMOVE_BATCH_SUFFIXES:\n",
    "        if name.endswith(suffix):\n",
    "            name = name[:-len(suffix)]\n",
    "    \n",
    "    # Apply case transformation\n",
    "    if BATCH_NAME_TRANSFORM == \"uppercase\":\n",
    "        name = name.upper()\n",
    "    elif BATCH_NAME_TRANSFORM == \"lowercase\":\n",
    "        name = name.lower()\n",
    "    elif BATCH_NAME_TRANSFORM == \"title\":\n",
    "        name = name.title()\n",
    "    \n",
    "    return name\n",
    "\n",
    "def should_include_video_file(file_path):\n",
    "    \"\"\"Check if video file meets size requirements\"\"\"\n",
    "    try:\n",
    "        size_bytes = file_path.stat().st_size\n",
    "        size_mb = size_bytes / (1024**2)\n",
    "        size_gb = size_bytes / (1024**3)\n",
    "        \n",
    "        if MIN_FILE_SIZE_MB > 0 and size_mb < MIN_FILE_SIZE_MB:\n",
    "            return False\n",
    "        \n",
    "        if MAX_FILE_SIZE_GB > 0 and size_gb > MAX_FILE_SIZE_GB:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    except:\n",
    "        return True  # If we can't check size, include it\n",
    "\n",
    "def get_target_directories():\n",
    "    \"\"\"Get list of directories to scan based on configuration\"\"\"\n",
    "    if TARGET_DIRECTORIES:\n",
    "        # Use specified target directories\n",
    "        targets = []\n",
    "        for target in TARGET_DIRECTORIES:\n",
    "            target_path = Path(target).expanduser().resolve()\n",
    "            if target_path.exists() and target_path.is_dir():\n",
    "                targets.append(target_path)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Target directory not found: {target}\")\n",
    "        return targets\n",
    "    else:\n",
    "        # Use default auto-detection logic\n",
    "        return [Path(\".\"), Path(\"..\")]\n",
    "\n",
    "def get_output_path(filename):\n",
    "    \"\"\"Get full output path for a file based on configuration\"\"\"\n",
    "    if SESSION_METADATA_LOCATION:\n",
    "        # Use session-specific location\n",
    "        session_path = Path(SESSION_METADATA_LOCATION).expanduser().resolve()\n",
    "        session_path.mkdir(parents=True, exist_ok=True)\n",
    "        return session_path / filename\n",
    "    else:\n",
    "        # Use configured output directory\n",
    "        output_path = Path(OUTPUT_DIRECTORY).expanduser().resolve()\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        return output_path / filename\n",
    "\n",
    "def get_metadata_paths():\n",
    "    \"\"\"Get all configured output file paths\"\"\"\n",
    "    return {\n",
    "        'metadata': get_output_path(METADATA_FILENAME),\n",
    "        'inventory': get_output_path(INVENTORY_FILENAME),\n",
    "        'workspace_info': get_output_path(WORKSPACE_INFO_FILENAME)\n",
    "    }\n",
    "\n",
    "# Update video extensions with additional formats\n",
    "ALL_VIDEO_EXTENSIONS = VIDEO_EXTENSIONS.union(set(ext.lower() for ext in ADDITIONAL_VIDEO_EXTENSIONS))\n",
    "\n",
    "print(f\"üéØ Will scan {len(get_target_directories())} target directories\")\n",
    "print(f\"üìπ Monitoring {len(ALL_VIDEO_EXTENSIONS)} video formats: {sorted(ALL_VIDEO_EXTENSIONS)}\")\n",
    "print(f\"üíæ Output files will be saved to:\")\n",
    "paths = get_metadata_paths()\n",
    "for file_type, path in paths.items():\n",
    "    print(f\"   {file_type}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Discovery functions ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "\n",
    "# Cell 2: Discovery Functions\n",
    "def discover_video_batches(search_path=\".\"):\n",
    "    \"\"\"Discover video batches based on parent directory structure\"\"\"\n",
    "    search_root = Path(search_path)\n",
    "    batches = defaultdict(list)\n",
    "    \n",
    "    print(f\"üîç Scanning for video batches in: {search_root.absolute()}\")\n",
    "    \n",
    "    # Scan all subdirectories for video files\n",
    "    for item in search_root.iterdir():\n",
    "        if item.is_dir() and not item.name.startswith('.'):\n",
    "            batch_name = item.name\n",
    "            videos = []\n",
    "            \n",
    "            # Find video files in this directory\n",
    "            for video_file in item.iterdir():\n",
    "                if video_file.is_file() and video_file.suffix.lower() in VIDEO_EXTENSIONS:\n",
    "                    videos.append({\n",
    "                        'filename': video_file.name,\n",
    "                        'path': str(video_file.relative_to(search_root)),\n",
    "                        'full_path': str(video_file.absolute()),\n",
    "                        'size_bytes': video_file.stat().st_size,\n",
    "                        'size_mb': round(video_file.stat().st_size / (1024**2), 2),\n",
    "                        'size_gb': round(video_file.stat().st_size / (1024**3), 3),\n",
    "                        'modified': datetime.fromtimestamp(video_file.stat().st_mtime).isoformat(),\n",
    "                        'batch': batch_name\n",
    "                    })\n",
    "            \n",
    "            if videos:\n",
    "                batches[batch_name] = videos\n",
    "                print(f\"üìÅ Found batch '{batch_name}': {len(videos)} videos\")\n",
    "    \n",
    "    return dict(batches)\n",
    "\n",
    "def scan_current_directory():\n",
    "    \"\"\"Scan current directory for video batches\"\"\"\n",
    "    return discover_video_batches(\".\")\n",
    "\n",
    "def scan_parent_directory():\n",
    "    \"\"\"Scan parent directory for video batches (useful when notebook is in subdirectory)\"\"\"\n",
    "    return discover_video_batches(\"..\")\n",
    "\n",
    "print(\"‚úÖ Discovery functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning for video batches in: B:\\Video\\StockFootage\\Batches\n",
      "üìÅ Found batch 'well_pump': 16 videos\n",
      "\n",
      "üéØ Discovered 1 video batches\n",
      "  üìÅ well_pump\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "\n",
    "# Cell 3: Run Discovery\n",
    "# First try current directory, then parent if no batches found\n",
    "# batches = scan_current_directory()\n",
    "batches = discover_video_batches(repo_root)\n",
    "\n",
    "if not batches:\n",
    "    print(\"üîÑ No batches in current directory, checking parent...\")\n",
    "    batches = scan_parent_directory()\n",
    "\n",
    "if not batches:\n",
    "    print(\"‚ùå No video batches found!\")\n",
    "    print(\"Expected structure:\")\n",
    "    print(\"  üìÅ BatchName1/\")\n",
    "    print(\"  ‚îú‚îÄ‚îÄ üé¨ video1.mp4\")\n",
    "    print(\"  ‚îî‚îÄ‚îÄ üé¨ video2.mp4\")\n",
    "else:\n",
    "    print(f\"\\nüéØ Discovered {len(batches)} video batches\")\n",
    "    for batch_name in batches.keys():\n",
    "        print(f\"  üìÅ {batch_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Batches Type: <class 'dict'>\n",
      "üì¶ Total Batches: 1\n",
      "  üìÅ well_pump: 16 videos\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Cell 3.5 (safe): Convert Path objects into metadata dicts only if needed\n",
    "\n",
    "def enrich_video_paths(batches_dict: dict[str, list]) -> dict[str, list[dict]]:\n",
    "    enriched = {}\n",
    "\n",
    "    for batch_name, videos in batches_dict.items():\n",
    "        # Skip enrichment if already dicts\n",
    "        if isinstance(videos[0], dict):\n",
    "            enriched[batch_name] = videos\n",
    "            continue\n",
    "\n",
    "        enriched[batch_name] = []\n",
    "        for video_path in videos:\n",
    "            enriched[batch_name].append({\n",
    "                'filename': video_path.name,\n",
    "                'filepath': str(video_path.resolve()),\n",
    "                'size_bytes': video_path.stat().st_size,\n",
    "                'modified': datetime.fromtimestamp(video_path.stat().st_mtime).isoformat(),\n",
    "                'created': datetime.fromtimestamp(video_path.stat().st_ctime).isoformat(),\n",
    "                'extension': video_path.suffix.lower()\n",
    "            })\n",
    "\n",
    "    return enriched\n",
    "\n",
    "# Only enrich if needed\n",
    "batches = enrich_video_paths(batches)\n",
    "\n",
    "print(\"üß™ Batches Type:\", type(batches))\n",
    "print(\"üì¶ Total Batches:\", len(batches))\n",
    "\n",
    "for batch, items in batches.items():\n",
    "    print(f\"  üìÅ {batch}: {len(items)} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Generated metadata for 1 batches\n",
      "   Total videos: 16\n",
      "   Total size: 8.07 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "\n",
    "# Cell 4: Generate Comprehensive Metadata\n",
    "def generate_batch_metadata(batches_dict):\n",
    "    \"\"\"Generate comprehensive metadata for all video batches\"\"\"\n",
    "    \n",
    "    metadata = {\n",
    "        'scan_time': datetime.now().isoformat(),\n",
    "        'notebook_location': str(repo_root.absolute()),\n",
    "        'total_batches': len(batches_dict),\n",
    "        'total_videos': sum(len(videos) for videos in batches_dict.values()),\n",
    "        'batches': {}\n",
    "    }\n",
    "    \n",
    "    for batch_name, videos in batches_dict.items():\n",
    "        total_size = sum(video['size_bytes'] for video in videos)\n",
    "        \n",
    "        metadata['batches'][batch_name] = {\n",
    "            'video_count': len(videos),\n",
    "            'total_size_bytes': total_size,\n",
    "            'total_size_mb': round(total_size / (1024**2), 2),\n",
    "            'total_size_gb': round(total_size / (1024**3), 2),\n",
    "            'videos': videos,\n",
    "            'batch_id': hashlib.md5(batch_name.encode()).hexdigest()[:8],\n",
    "            'first_video': videos[0]['filename'] if videos else None,\n",
    "            'last_modified': max(video['modified'] for video in videos) if videos else None\n",
    "        }\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Generate metadata\n",
    "if batches:\n",
    "    metadata = generate_batch_metadata(batches)\n",
    "    print(f\"üìä Generated metadata for {metadata['total_batches']} batches\")\n",
    "    print(f\"   Total videos: {metadata['total_videos']}\")\n",
    "    print(f\"   Total size: {sum(b['total_size_gb'] for b in metadata['batches'].values()):.2f} GB\")\n",
    "else:\n",
    "    metadata = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Created DataFrame with 16 video records\n",
      "\n",
      "üîç Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_name</th>\n",
       "      <th>filename</th>\n",
       "      <th>size_mb</th>\n",
       "      <th>size_gb</th>\n",
       "      <th>modified_date</th>\n",
       "      <th>path</th>\n",
       "      <th>full_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>well_pump</td>\n",
       "      <td>Z7V_1648.MP4</td>\n",
       "      <td>148.69</td>\n",
       "      <td>0.145</td>\n",
       "      <td>2025-06-04 12:27:32.075202</td>\n",
       "      <td>well_pump\\Z7V_1648.MP4</td>\n",
       "      <td>B:\\Video\\StockFootage\\Batches\\well_pump\\Z7V_16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>well_pump</td>\n",
       "      <td>Z7V_1649.MP4</td>\n",
       "      <td>142.93</td>\n",
       "      <td>0.140</td>\n",
       "      <td>2025-06-04 12:27:32.132411</td>\n",
       "      <td>well_pump\\Z7V_1649.MP4</td>\n",
       "      <td>B:\\Video\\StockFootage\\Batches\\well_pump\\Z7V_16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well_pump</td>\n",
       "      <td>Z7V_1650.MP4</td>\n",
       "      <td>73.29</td>\n",
       "      <td>0.072</td>\n",
       "      <td>2025-06-04 12:27:32.178865</td>\n",
       "      <td>well_pump\\Z7V_1650.MP4</td>\n",
       "      <td>B:\\Video\\StockFootage\\Batches\\well_pump\\Z7V_16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>well_pump</td>\n",
       "      <td>Z7V_1651.MP4</td>\n",
       "      <td>408.93</td>\n",
       "      <td>0.399</td>\n",
       "      <td>2025-06-04 12:27:32.234802</td>\n",
       "      <td>well_pump\\Z7V_1651.MP4</td>\n",
       "      <td>B:\\Video\\StockFootage\\Batches\\well_pump\\Z7V_16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>well_pump</td>\n",
       "      <td>Z7V_1652.MP4</td>\n",
       "      <td>487.64</td>\n",
       "      <td>0.476</td>\n",
       "      <td>2025-06-04 12:27:32.291694</td>\n",
       "      <td>well_pump\\Z7V_1652.MP4</td>\n",
       "      <td>B:\\Video\\StockFootage\\Batches\\well_pump\\Z7V_16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  batch_name      filename  size_mb  size_gb              modified_date  \\\n",
       "0  well_pump  Z7V_1648.MP4   148.69    0.145 2025-06-04 12:27:32.075202   \n",
       "1  well_pump  Z7V_1649.MP4   142.93    0.140 2025-06-04 12:27:32.132411   \n",
       "2  well_pump  Z7V_1650.MP4    73.29    0.072 2025-06-04 12:27:32.178865   \n",
       "3  well_pump  Z7V_1651.MP4   408.93    0.399 2025-06-04 12:27:32.234802   \n",
       "4  well_pump  Z7V_1652.MP4   487.64    0.476 2025-06-04 12:27:32.291694   \n",
       "\n",
       "                     path                                          full_path  \n",
       "0  well_pump\\Z7V_1648.MP4  B:\\Video\\StockFootage\\Batches\\well_pump\\Z7V_16...  \n",
       "1  well_pump\\Z7V_1649.MP4  B:\\Video\\StockFootage\\Batches\\well_pump\\Z7V_16...  \n",
       "2  well_pump\\Z7V_1650.MP4  B:\\Video\\StockFootage\\Batches\\well_pump\\Z7V_16...  \n",
       "3  well_pump\\Z7V_1651.MP4  B:\\Video\\StockFootage\\Batches\\well_pump\\Z7V_16...  \n",
       "4  well_pump\\Z7V_1652.MP4  B:\\Video\\StockFootage\\Batches\\well_pump\\Z7V_16...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_count</th>\n",
       "      <th>total_gb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>well_pump</th>\n",
       "      <td>16</td>\n",
       "      <td>8.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            video_count  total_gb\n",
       "batch_name                       \n",
       "well_pump            16      8.07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "\n",
    "# Cell 5: Create DataFrame for Analysis\n",
    "def create_video_dataframe(batches_dict):\n",
    "    \"\"\"Create a pandas DataFrame from video batch data\"\"\"\n",
    "    all_videos = []\n",
    "    \n",
    "    for batch_name, videos in batches_dict.items():\n",
    "        for video in videos:\n",
    "            all_videos.append({\n",
    "                'batch_name': batch_name,\n",
    "                'filename': video['filename'],\n",
    "                'size_mb': video['size_mb'],\n",
    "                'size_gb': video['size_gb'],\n",
    "                'modified_date': pd.to_datetime(video['modified']),\n",
    "                'path': video['path'],\n",
    "                'full_path': video['full_path']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(all_videos)\n",
    "\n",
    "if batches:\n",
    "    df_videos = create_video_dataframe(batches)\n",
    "    print(f\"üìà Created DataFrame with {len(df_videos)} video records\")\n",
    "    print(\"\\nüîç Sample data:\")\n",
    "    display(df_videos.head())\n",
    "    \n",
    "    print(\"\\nüìä Batch summary:\")\n",
    "    batch_summary = df_videos.groupby('batch_name').agg({\n",
    "        'filename': 'count',\n",
    "        'size_gb': 'sum'\n",
    "    }).round(2)\n",
    "    batch_summary.columns = ['video_count', 'total_gb']\n",
    "    display(batch_summary)\n",
    "else:\n",
    "    df_videos = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Video For Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Setup captioning model (ViT is smaller, works well for single-frame)\n",
    "captioner = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\", device=-1)  # CPU\n",
    "\n",
    "def extract_middle_frame(video_path):\n",
    "    \"\"\"Extracts the middle frame from the video for analysis.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_idx = total_frames // 2\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        return None\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(frame_rgb)\n",
    "    return pil_image\n",
    "\n",
    "def generate_caption_and_keywords(pil_image):\n",
    "    \"\"\"Uses ViT pipeline to generate description and keywords.\"\"\"\n",
    "    result = captioner(pil_image)\n",
    "    caption = result[0]['generated_text']\n",
    "    # Keywords: most frequent nouns or unique words longer than 3 chars\n",
    "    words = caption.replace('.', '').lower().split()\n",
    "    keywords = [w for w in words if len(w) > 3]\n",
    "    # Deduplicate and limit to ~8 for Blackbox\n",
    "    keywords = list(dict.fromkeys(keywords))[:8]\n",
    "    return caption, ', '.join(keywords)\n",
    "\n",
    "# 2. Add new columns to df_videos\n",
    "descriptions = []\n",
    "keywords_list = []\n",
    "\n",
    "for idx, row in df_videos.iterrows():\n",
    "    full_path = row['full_path']  # ensure you have the path column!\n",
    "    img = extract_middle_frame(full_path)\n",
    "    if img is not None:\n",
    "        desc, kws = generate_caption_and_keywords(img)\n",
    "    else:\n",
    "        desc, kws = \"Could not extract frame.\", \"\"\n",
    "    descriptions.append(desc)\n",
    "    keywords_list.append(kws)\n",
    "\n",
    "df_videos['Description'] = descriptions\n",
    "df_videos['Keywords'] = keywords_list\n",
    "# Optionally rename for XML output\n",
    "df_videos['Filename'] = df_videos['filename']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording Above Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved metadata to: C:\\Users\\david\\JupyterLab\\stock-video-metadata\\batch_metadata.json\n",
      "üíæ Saved video inventory to: C:\\Users\\david\\JupyterLab\\stock-video-metadata\\video_inventory.csv\n",
      "üíæ Saved workspace info to: C:\\Users\\david\\JupyterLab\\stock-video-metadata\\workspace_info.json\n",
      "üì∏ Session snapshot saved to: C:\\Users\\david\\JupyterLab\\stock-video-metadata\\session_snapshot_20250609_122156.json\n",
      "‚úÖ All data saved successfully!\n",
      "üìÇ Files saved to: C:\\Users\\david\\JupyterLab\\stock-video-metadata\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Save Results (Enhanced with configuration support)\n",
    "def save_batch_data(metadata, df_videos=None):\n",
    "    \"\"\"Save batch metadata and optionally DataFrame to configured locations\"\"\"\n",
    "    \n",
    "    # Get configured file paths\n",
    "    paths = get_metadata_paths()\n",
    "    \n",
    "    # Save metadata JSON\n",
    "    with open(paths['metadata'], 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"üíæ Saved metadata to: {paths['metadata']}\")\n",
    "    \n",
    "    # Save DataFrame as CSV if available\n",
    "    if df_videos is not None:\n",
    "        df_videos.to_csv(paths['inventory'], index=False)\n",
    "        print(f\"üíæ Saved video inventory to: {paths['inventory']}\")\n",
    "    \n",
    "    # Create workspace info\n",
    "    workspace_info = {\n",
    "        'workspace_type': 'video_batching',\n",
    "        'structure': 'parent_directory_batching',\n",
    "        'notebook_location': str(repo_root),\n",
    "        'session_metadata_location': str(SESSION_METADATA_LOCATION) if SESSION_METADATA_LOCATION else None,\n",
    "        'output_directory': str(OUTPUT_DIRECTORY),\n",
    "        'batches_discovered': list(metadata['batches'].keys()) if metadata else [],\n",
    "        'ready_for_processing': len(metadata['batches']) > 0 if metadata else False,\n",
    "        'last_scan': datetime.now().isoformat(),\n",
    "        'configuration': {\n",
    "            'target_directories': TARGET_DIRECTORIES,\n",
    "            'include_patterns': INCLUDE_BATCH_NAMES,\n",
    "            'ignore_patterns': IGNORE_DIRECTORY_NAMES,\n",
    "            'file_size_limits': {\n",
    "                'min_mb': MIN_FILE_SIZE_MB,\n",
    "                'max_gb': MAX_FILE_SIZE_GB\n",
    "            },\n",
    "            'batch_naming': {\n",
    "                'transform': BATCH_NAME_TRANSFORM,\n",
    "                'remove_prefixes': REMOVE_BATCH_PREFIXES,\n",
    "                'remove_suffixes': REMOVE_BATCH_SUFFIXES\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(paths['workspace_info'], 'w') as f:\n",
    "        json.dump(workspace_info, f, indent=2)\n",
    "    print(f\"üíæ Saved workspace info to: {paths['workspace_info']}\")\n",
    "    \n",
    "    return paths\n",
    "\n",
    "def save_session_snapshot():\n",
    "    \"\"\"Save a quick session snapshot with timestamp\"\"\"\n",
    "    if metadata:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        snapshot_filename = f\"session_snapshot_{timestamp}.json\"\n",
    "        snapshot_path = get_output_path(snapshot_filename)\n",
    "        \n",
    "        snapshot_data = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'session_id': timestamp,\n",
    "            'batch_summary': {\n",
    "                batch_name: {\n",
    "                    'video_count': len(videos),\n",
    "                    'total_gb': round(sum(v['size_gb'] for v in videos), 2)\n",
    "                }\n",
    "                for batch_name, videos in batches.items()\n",
    "            },\n",
    "            'total_batches': len(batches),\n",
    "            'total_videos': sum(len(videos) for videos in batches.values()),\n",
    "            'configuration_used': {\n",
    "                'target_dirs': TARGET_DIRECTORIES,\n",
    "                'session_location': SESSION_METADATA_LOCATION\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(snapshot_path, 'w') as f:\n",
    "            json.dump(snapshot_data, f, indent=2)\n",
    "        print(f\"üì∏ Session snapshot saved to: {snapshot_path}\")\n",
    "        return snapshot_path\n",
    "    return None\n",
    "\n",
    "if metadata:\n",
    "    saved_files = save_batch_data(metadata, df_videos)\n",
    "    snapshot_file = save_session_snapshot()\n",
    "    print(\"‚úÖ All data saved successfully!\")\n",
    "    print(f\"üìÇ Files saved to: {paths['metadata'].parent}\")\n",
    "else:\n",
    "    print(\"‚ùå No data to save - no batches found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Interactive Analysis Ready!\n",
      "\n",
      "Available functions:\n",
      "‚Ä¢ list_all_batches() - Show all batches\n",
      "‚Ä¢ analyze_batch('batch_name') - Analyze specific batch\n",
      "‚Ä¢ df_videos - Pandas DataFrame with all video data\n",
      "‚Ä¢ metadata - Complete metadata dictionary\n",
      "üìÅ All discovered batches:\n",
      "   well_pump: 16 videos, 8.07 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "\n",
    "# Cell 7: Interactive Analysis Functions\n",
    "def analyze_batch(batch_name):\n",
    "    \"\"\"Analyze a specific batch\"\"\"\n",
    "    if batch_name not in batches:\n",
    "        print(f\"‚ùå Batch '{batch_name}' not found\")\n",
    "        return\n",
    "    \n",
    "    videos = batches[batch_name]\n",
    "    print(f\"üé¨ Analyzing batch: {batch_name}\")\n",
    "    print(f\"   Videos: {len(videos)}\")\n",
    "    print(f\"   Total size: {sum(v['size_gb'] for v in videos):.2f} GB\")\n",
    "    print(f\"   Average size: {sum(v['size_mb'] for v in videos) / len(videos):.1f} MB\")\n",
    "    \n",
    "    print(\"\\nüìã Video list:\")\n",
    "    for video in videos:\n",
    "        print(f\"   ‚Ä¢ {video['filename']} ({video['size_mb']} MB)\")\n",
    "\n",
    "def list_all_batches():\n",
    "    \"\"\"List all discovered batches with summary info\"\"\"\n",
    "    if not batches:\n",
    "        print(\"‚ùå No batches found\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìÅ All discovered batches:\")\n",
    "    for batch_name, videos in batches.items():\n",
    "        total_gb = sum(v['size_gb'] for v in videos)\n",
    "        print(f\"   {batch_name}: {len(videos)} videos, {total_gb:.2f} GB\")\n",
    "\n",
    "# Run interactive analysis\n",
    "if batches:\n",
    "    print(\"üéØ Interactive Analysis Ready!\")\n",
    "    print(\"\\nAvailable functions:\")\n",
    "    print(\"‚Ä¢ list_all_batches() - Show all batches\")\n",
    "    print(\"‚Ä¢ analyze_batch('batch_name') - Analyze specific batch\")\n",
    "    print(\"‚Ä¢ df_videos - Pandas DataFrame with all video data\")\n",
    "    print(\"‚Ä¢ metadata - Complete metadata dictionary\")\n",
    "    \n",
    "    # Auto-run summary\n",
    "    list_all_batches()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No batches available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Utility functions loaded:\n",
      "‚Ä¢ get_batch_paths(batch_name) - Get file paths for batch\n",
      "‚Ä¢ get_largest_videos(n=5) - Find largest videos\n",
      "‚Ä¢ get_recent_videos(days=7) - Find recently modified videos\n",
      "\n",
      "üîç Example - 3 largest videos:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_name</th>\n",
       "      <th>filename</th>\n",
       "      <th>size_gb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>well_pump</td>\n",
       "      <td>Z7V_1657.MP4</td>\n",
       "      <td>1.589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>well_pump</td>\n",
       "      <td>Z7V_1654.MP4</td>\n",
       "      <td>1.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>well_pump</td>\n",
       "      <td>Z7V_1659.MP4</td>\n",
       "      <td>1.028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_name      filename  size_gb\n",
       "9   well_pump  Z7V_1657.MP4    1.589\n",
       "6   well_pump  Z7V_1654.MP4    1.046\n",
       "11  well_pump  Z7V_1659.MP4    1.028"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Video batch processing notebook ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "\n",
    "# Cell 8: Utility Functions for Further Processing\n",
    "def get_batch_paths(batch_name):\n",
    "    \"\"\"Get all video file paths for a specific batch\"\"\"\n",
    "    if batch_name not in batches:\n",
    "        return []\n",
    "    return [video['full_path'] for video in batches[batch_name]]\n",
    "\n",
    "def get_largest_videos(n=5):\n",
    "    \"\"\"Get the N largest video files across all batches\"\"\"\n",
    "    if df_videos is not None:\n",
    "        return df_videos.nlargest(n, 'size_gb')[['batch_name', 'filename', 'size_gb']]\n",
    "    return None\n",
    "\n",
    "def get_recent_videos(days=7):\n",
    "    \"\"\"Get videos modified in the last N days\"\"\"\n",
    "    if df_videos is not None:\n",
    "        cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=days)\n",
    "        recent = df_videos[df_videos['modified_date'] > cutoff_date]\n",
    "        return recent[['batch_name', 'filename', 'modified_date', 'size_gb']]\n",
    "    return None\n",
    "\n",
    "print(\"üõ†Ô∏è Utility functions loaded:\")\n",
    "print(\"‚Ä¢ get_batch_paths(batch_name) - Get file paths for batch\")\n",
    "print(\"‚Ä¢ get_largest_videos(n=5) - Find largest videos\")\n",
    "print(\"‚Ä¢ get_recent_videos(days=7) - Find recently modified videos\")\n",
    "\n",
    "if df_videos is not None:\n",
    "    print(f\"\\nüîç Example - 3 largest videos:\")\n",
    "    display(get_largest_videos(3))\n",
    "\n",
    "print(\"\\n‚úÖ Video batch processing notebook ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Connecting to Davinci Resolve Network Project Postgres Data\n",
    "\n",
    "```python\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. load your JSON metadata\n",
    "meta = json.load(open(Path(__file__).parent.parent / \"batch_metadata.json\"))\n",
    "\n",
    "# 2. grab the Resolve scripting API\n",
    "import DaVinciResolveScript as bmd\n",
    "resolve = bmd.scriptapp(\"Resolve\")\n",
    "pm = resolve.GetProjectManager()\n",
    "proj = pm.LoadProject(\"StockFootage Library\")\n",
    "mp = proj.GetMediaPool()\n",
    "\n",
    "# 3. for each batch, make a bin & import its clips\n",
    "for batch_name, info in meta[\"batches\"].items():\n",
    "    # create (or reuse) a bin\n",
    "    root = mp.GetRootFolder()\n",
    "    try:\n",
    "        bin = mp.AddSubFolder(root, batch_name)\n",
    "    except:\n",
    "        # already exists?\n",
    "        bin = next(b for b in root.GetSubFolders() if b.GetName() == batch_name)\n",
    "\n",
    "    # import media into that bin\n",
    "    paths = [v[\"full_path\"] for v in info[\"videos\"]]\n",
    "    mp.AddItemListToMediaPool(paths)\n",
    "\n",
    "    # tag each clip with metadata\n",
    "    for clip in bin.GetClipList():\n",
    "        # set the ‚ÄúScene‚Äù field to your batch name\n",
    "        clip.SetClipProperty(\"Scene\", batch_name)\n",
    "        # set the ‚ÄúComment‚Äù field to maybe your description or tags\n",
    "        clip.SetClipProperty(\"Comment\", f\"{info['video_count']} clips, total {info['total_size_gb']:.2f} GB\")\n",
    "\n",
    "# 4. save the project\n",
    "proj.Save()\n",
    "print(\"‚úÖ Ingest + tagging complete.\")\n",
    "```\n",
    "\n",
    "### Above Code Block:\n",
    "- /scripts/ingest_to_resolve.py\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Blackbox Global's `metadata.xml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "template_path = \"BlackBox Metadata Template MAY 10 2024.xlsx\"\n",
    "\n",
    "# Load the main template sheet\n",
    "df_meta = pd.read_excel(template_path, sheet_name=\"template\")\n",
    "\n",
    "# (Optional) load dropdown values for validation\n",
    "df_dd = pd.read_excel(template_path, sheet_name=\"dataDropdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Excel metadata updated: BlackBox Metadata Template MAY 10 2024.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "def generate_blackbox_rows(df_videos):\n",
    "    rows = []\n",
    "\n",
    "    for _, row in df_videos.iterrows():\n",
    "        # Build relative filename (Blackbox uses forward slashes)\n",
    "        file_path = f\"{row['batch_name']}/{row['filename']}\".replace(\"\\\\\", \"/\")\n",
    "\n",
    "        # Auto-fill with dummy data (replace with real data or LLM suggestions later)\n",
    "        bb_row = {\n",
    "            \"File Name\": file_path,\n",
    "            \"Description (min 15, max 200 characters, must be least 5 words)\": \"A cinematic wide-angle shot of a well pump in rural Georgia.\",\n",
    "            \"Keywords (min 8, max 49, separated by comma, and no repetition)\": \"rural, well pump, metal, outdoor, agriculture, infrastructure, water, day\",\n",
    "            \"Category (use dropdown menu)\": \"Infrastructure\",\n",
    "            \"Editorial (use dropdown menu)\": False,\n",
    "            # Add more columns if needed from df_meta.columns\n",
    "        }\n",
    "        rows.append(bb_row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_bb = generate_blackbox_rows(df_videos)\n",
    "\n",
    "# Append to the original template\n",
    "df_meta = pd.concat([df_meta, df_bb], ignore_index=True)\n",
    "\n",
    "with pd.ExcelWriter(template_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n",
    "    df_meta.to_excel(writer, sheet_name=\"template\", index=False)\n",
    "print(f\"üíæ Excel metadata updated: {template_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exported metadata.xml for BlackBox Global\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "root = ET.Element(\"MediaMetaData\")\n",
    "\n",
    "for _, row in df_meta.iterrows():\n",
    "    clip = ET.SubElement(root, \"Clip\")\n",
    "    for col in df_meta.columns:\n",
    "        xml_tag = col.replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\").replace(\"-\", \"\")\n",
    "        child = ET.SubElement(clip, xml_tag)\n",
    "        child.text = str(row[col])\n",
    "\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(\"metadata.xml\", encoding=\"utf-8\", xml_declaration=True)\n",
    "print(\"‚úÖ Exported metadata.xml for BlackBox Global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: assembling one row\n",
    "new_row = {\n",
    "    \"File Name\": \"Z7V_1641/video1.mp4\",\n",
    "    \"Description (min 15, max 200 characters, must be least 5 words)\": \"A wide shot of...\",\n",
    "    \"Keywords (min 8, max 49, separated by comma, and no repetition)\": \"drone, landscape, 4k, nature, wide, aerial, motion, shot\",\n",
    "    \"Category (use dropdown menu)\": \"Nature\",\n",
    "    \"Editorial (use dropdown menu)\": False,\n",
    "    # etc‚Ä¶ fill every required column\n",
    "}\n",
    "df_meta = df_meta.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With this flow you‚Äôll have:\n",
    "\n",
    "- excel‚Äêdriven human‚Äêfriendly template for metadata entry & validation\n",
    "- automated sync from your Python pipeline into that template\n",
    "- XML export for BlackBox Global ingestion\n",
    "- (Optional) Postgres backend feeding DaVinci Resolve‚Äôs shared library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publishing Metadata & Footage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep needed columns, and save to CSV or XML\n",
    "export_df = df_videos[['Filename', 'Description', 'Keywords']]\n",
    "\n",
    "# CSV (most reliable for spreadsheet import)\n",
    "export_path = \"blackbox_metadata.csv\"\n",
    "export_df.to_csv(export_path, index=False)\n",
    "print(f\"‚úÖ Blackbox CSV Metadata Created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV => XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Export BlackBox-Ready CSV and XML\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# 1) CSV export\n",
    "export_csv = \"blackbox_metadata.csv\"\n",
    "df_bb = df_videos[['Filename', 'Description', 'Keywords']]\n",
    "df_bb.to_csv(export_csv, index=False)\n",
    "print(f\"üíæ Exported BlackBox CSV: {export_csv}\")\n",
    "\n",
    "# 2) Per‚Äêbatch XML export\n",
    "# Assumes you have one metadata.xml per batch in each batch folder\n",
    "for batch_name, group in df_videos.groupby('batch_name'):\n",
    "    # Create root\n",
    "    root = ET.Element(\"MediaMetaData\")\n",
    "    for _, row in group.iterrows():\n",
    "        clip = ET.SubElement(root, \"Clip\")\n",
    "        ET.SubElement(clip, \"Filename\").text    = row['Filename']\n",
    "        ET.SubElement(clip, \"Description\").text = row['Description']\n",
    "        ET.SubElement(clip, \"Keywords\").text    = row['Keywords']\n",
    "    tree = ET.ElementTree(root)\n",
    "    xml_path = repo_root / batch_name / \"metadata.xml\"\n",
    "    tree.write(xml_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "    print(f\"üíæ Wrote XML for batch '{batch_name}': {xml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFTP/FTP To Blackbox Global Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Upload videos + XML to BlackBox via SFTP\n",
    "\n",
    "!pip install paramiko\n",
    "\n",
    "import paramiko\n",
    "from pathlib import Path\n",
    "\n",
    "# ‚Äî Load SFTP credentials (could also come from .env)\n",
    "SFTP_HOST     = \"sftp.blackboxglobal.com\"\n",
    "SFTP_PORT     = 22\n",
    "SFTP_USER     = \"your_username\"\n",
    "SFTP_PASS     = \"your_password\"\n",
    "REMOTE_ROOT   = \"/incoming/videos\"    # base path on the SFTP server\n",
    "\n",
    "def sftp_connect():\n",
    "    transport = paramiko.Transport((SFTP_HOST, SFTP_PORT))\n",
    "    transport.connect(username=SFTP_USER, password=SFTP_PASS)\n",
    "    return paramiko.SFTPClient.from_transport(transport)\n",
    "\n",
    "def upload_batch(batch_name, sftp: paramiko.SFTPClient):\n",
    "    local_batch = repo_root / batch_name\n",
    "    remote_batch = f\"{REMOTE_ROOT}/{batch_name}\"\n",
    "    try:\n",
    "        sftp.mkdir(remote_batch)\n",
    "    except IOError:\n",
    "        pass  # already exists\n",
    "    # Upload metadata.xml\n",
    "    local_xml = local_batch / \"metadata.xml\"\n",
    "    sftp.put(str(local_xml), f\"{remote_batch}/metadata.xml\")\n",
    "    # Upload each video\n",
    "    for video_file in (local_batch).glob(\"*.*\"):\n",
    "        if video_file.suffix.lower() in VIDEO_EXTENSIONS:\n",
    "            remote_path = f\"{remote_batch}/{video_file.name}\"\n",
    "            sftp.put(str(video_file), remote_path)\n",
    "            print(f\"   üì§ Uploaded {video_file.name}\")\n",
    "    print(f\"‚úÖ Batch '{batch_name}' uploaded to {remote_batch}\")\n",
    "\n",
    "# Run upload for all discovered batches\n",
    "sftp = sftp_connect()\n",
    "for batch_name in df_videos['batch_name'].unique():\n",
    "    print(f\"Uploading batch: {batch_name}\")\n",
    "    upload_batch(batch_name, sftp)\n",
    "sftp.close()\n",
    "print(\"üéâ All batches & metadata uploaded successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
